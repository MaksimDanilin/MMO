{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Лабораторная работа №4 по курсу \"Методы машинного обучения\"\n",
        "\n",
        "# **ИУ5-21М Данилин М.С.**"
      ],
      "metadata": {
        "id": "NmTD8vmgJ8Xt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tBAt87PJw8C"
      },
      "source": [
        "- **Цель лабораторной работы**: ознакомление с базовыми методами обучения с подкреплением"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uBatD1jJw8D"
      },
      "source": [
        "# Задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmSr1WtWJw8D"
      },
      "source": [
        "- На основе рассмотренного на лекции примера реализуйте алгоритм Policy Iteration для любой среды обучения с подкреплением (кроме рассмотренной на лекции среды Toy Text / Frozen Lake) из библиотеки Gym (или аналогичной библиотеки)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egz2mrc5Jw8D"
      },
      "source": [
        "# Подключение библиотек"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install gym==0.26.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0qd-HdYK4KB",
        "outputId": "a76c5d47-567a-4fa7-f606-ddafcf11412d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: gym==0.26.2 in /usr/local/lib/python3.11/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r2cF7-jkJw8E"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YSnBdYeJw8F"
      },
      "source": [
        "# Ход работы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PHUq2U0Jw8F",
        "outputId": "c60f05d2-4bc7-4b98-be34-15013d036d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Стратегия:\n",
            "array([[0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25],\n",
            "       [0.25, 0.25, 0.25, 0.25]])\n",
            "Алгоритм выполнился за 1000 шагов.\n",
            "Стратегия:\n",
            "array([[0.        , 0.5       , 0.5       , 0.        ],\n",
            "       [0.33333333, 0.33333333, 0.33333333, 0.        ],\n",
            "       [0.        , 0.        , 1.        , 0.        ],\n",
            "       [0.        , 0.        , 1.        , 0.        ],\n",
            "       [0.        , 0.        , 1.        , 0.        ],\n",
            "       [0.        , 0.        , 1.        , 0.        ],\n",
            "       [0.        , 0.        , 1.        , 0.        ],\n",
            "       [0.        , 0.        , 1.        , 0.        ],\n",
            "       [0.        , 0.        , 1.        , 0.        ],\n",
            "       [0.        , 0.        , 1.        , 0.        ],\n",
            "       [0.33333333, 0.        , 0.33333333, 0.33333333],\n",
            "       [0.        , 0.        , 0.5       , 0.5       ],\n",
            "       [0.        , 0.        , 1.        , 0.        ],\n",
            "       [0.        , 0.5       , 0.5       , 0.        ],\n",
            "       [0.        , 0.5       , 0.5       , 0.        ],\n",
            "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
            "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
            "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
            "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
            "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
            "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
            "       [0.        , 0.        , 0.5       , 0.5       ],\n",
            "       [0.        , 0.        , 0.5       , 0.5       ],\n",
            "       [0.        , 0.        , 1.        , 0.        ],\n",
            "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
            "       [0.        , 0.5       , 0.        , 0.5       ],\n",
            "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
            "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
            "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
            "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
            "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
            "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
            "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
            "       [0.33333333, 0.33333333, 0.        , 0.33333333],\n",
            "       [0.        , 0.5       , 0.        , 0.5       ],\n",
            "       [0.        , 0.33333333, 0.33333333, 0.33333333],\n",
            "       [0.33333333, 0.        , 0.33333333, 0.33333333],\n",
            "       [0.5       , 0.        , 0.        , 0.5       ],\n",
            "       [1.        , 0.        , 0.        , 0.        ],\n",
            "       [1.        , 0.        , 0.        , 0.        ],\n",
            "       [1.        , 0.        , 0.        , 0.        ],\n",
            "       [1.        , 0.        , 0.        , 0.        ],\n",
            "       [1.        , 0.        , 0.        , 0.        ],\n",
            "       [1.        , 0.        , 0.        , 0.        ],\n",
            "       [1.        , 0.        , 0.        , 0.        ],\n",
            "       [1.        , 0.        , 0.        , 0.        ],\n",
            "       [0.5       , 0.5       , 0.        , 0.        ],\n",
            "       [0.33333333, 0.33333333, 0.33333333, 0.        ]])\n"
          ]
        }
      ],
      "source": [
        "class PolicyIterationAgent:\n",
        "    '''\n",
        "    Класс, эмулирующий работу агента\n",
        "    '''\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        # Пространство состояний\n",
        "        self.observation_dim = 48\n",
        "        # Массив действий в соответствии с документацией\n",
        "        self.actions_variants = np.array([3,2,1,0])\n",
        "        # Задание стратегии (политики)\n",
        "        # Карта 12х4 и 4 возможных действия\n",
        "        self.policy_probs = np.full((self.observation_dim, len(self.actions_variants)), 0.25)\n",
        "        # Начальные значения для v(s)\n",
        "        self.state_values = np.zeros(shape=(self.observation_dim))\n",
        "        # Начальные значения параметров\n",
        "        self.maxNumberOfIterations = 1000\n",
        "        self.theta=1e-6\n",
        "        self.gamma=0.99\n",
        "\n",
        "\n",
        "    def print_policy(self):\n",
        "        '''\n",
        "        Вывод матриц стратегии\n",
        "        '''\n",
        "        print('Стратегия:')\n",
        "        pprint(self.policy_probs)\n",
        "\n",
        "\n",
        "    def policy_evaluation(self):\n",
        "        '''\n",
        "        Оценивание стратегии\n",
        "        '''\n",
        "        # Предыдущее значение функции ценности\n",
        "        valueFunctionVector = self.state_values\n",
        "        for iterations in range(self.maxNumberOfIterations):\n",
        "            # Новое значение функции ценности\n",
        "            valueFunctionVectorNextIteration=np.zeros(shape=(self.observation_dim))\n",
        "            # Цикл по состояниям\n",
        "            for state in range(self.observation_dim):\n",
        "                # Вероятности действий\n",
        "                action_probabilities = self.policy_probs[state]\n",
        "                # Цикл по действиям\n",
        "                outerSum=0\n",
        "                for action, prob in enumerate(action_probabilities):\n",
        "                    innerSum=0\n",
        "                    # Цикл по вероятностям действий\n",
        "                    for probability, next_state, reward, isTerminalState in self.env.P[state][action]:\n",
        "                        innerSum=innerSum+probability*(reward+self.gamma*self.state_values[next_state])\n",
        "                    outerSum=outerSum+self.policy_probs[state][action]*innerSum\n",
        "                valueFunctionVectorNextIteration[state]=outerSum\n",
        "            if(np.max(np.abs(valueFunctionVectorNextIteration-valueFunctionVector))<self.theta):\n",
        "                # Проверка сходимости алгоритма\n",
        "                valueFunctionVector=valueFunctionVectorNextIteration\n",
        "                break\n",
        "            valueFunctionVector=valueFunctionVectorNextIteration\n",
        "        return valueFunctionVector\n",
        "\n",
        "\n",
        "    def policy_improvement(self):\n",
        "        '''\n",
        "        Улучшение стратегии\n",
        "        '''\n",
        "        qvaluesMatrix=np.zeros((self.observation_dim, len(self.actions_variants)))\n",
        "        improvedPolicy=np.zeros((self.observation_dim, len(self.actions_variants)))\n",
        "        # Цикл по состояниям\n",
        "        for state in range(self.observation_dim):\n",
        "            for action in range(len(self.actions_variants)):\n",
        "                for probability, next_state, reward, isTerminalState in self.env.P[state][action]:\n",
        "                    qvaluesMatrix[state,action]=qvaluesMatrix[state,action]+probability*(reward+self.gamma*self.state_values[next_state])\n",
        "\n",
        "            # Находим лучшие индексы\n",
        "            bestActionIndex=np.where(qvaluesMatrix[state,:]==np.max(qvaluesMatrix[state,:]))\n",
        "            # Обновление стратегии\n",
        "            improvedPolicy[state,bestActionIndex]=1/np.size(bestActionIndex)\n",
        "        return improvedPolicy\n",
        "\n",
        "\n",
        "    def policy_iteration(self, cnt):\n",
        "        '''\n",
        "        Основная реализация алгоритма\n",
        "        '''\n",
        "        policy_stable = False\n",
        "        for i in range(1, cnt+1):\n",
        "            self.state_values = self.policy_evaluation()\n",
        "            self.policy_probs = self.policy_improvement()\n",
        "        print(f'Алгоритм выполнился за {i} шагов.')\n",
        "\n",
        "\n",
        "def play_agent(agent):\n",
        "    env2 = gym.make('CliffWalking-v0', render_mode='human')\n",
        "    state, info = env2.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        p = agent.policy_probs[state]\n",
        "        if isinstance(p, np.ndarray):\n",
        "            action = np.random.choice(len(agent.actions_variants), p=p)\n",
        "        else:\n",
        "            action = p\n",
        "        next_state, reward, terminated, truncated, info = env2.step(action)\n",
        "        env2.render()\n",
        "        state = next_state\n",
        "        if terminated or truncated:\n",
        "            done = True\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Создание среды\n",
        "    env = gym.make('CliffWalking-v0')\n",
        "    env.reset()\n",
        "    # Обучение агента\n",
        "    agent = PolicyIterationAgent(env)\n",
        "    agent.print_policy()\n",
        "    agent.policy_iteration(1000)\n",
        "    agent.print_policy()\n",
        "    # Проигрывание сцены для обученного агента\n",
        "    play_agent(agent)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}